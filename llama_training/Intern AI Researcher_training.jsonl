{"instruction": "Given the following job description, create a tailored resume:\n\nJob Title: Intern AI Researcher\n\nJob Description:\nWe are looking for a motivated and passionate student who has the following experience:\nStrong understanding of transformer-based models and large language models (LLMs) (e.g., GPT, LLaMA, Claude, etc.)\n\u2022 Solid experience with Python and libraries such as Hugging Face Transformers, LangChain, or LangGraph\n\u2022 Familiarity with prompt engineering and evaluation techniques (e.g., few-shot prompting, chain-of-thought, RAG, LLM-as-judge)\n\u2022 Hands-on experience with OpenAI API, Gemini, or other foundation models\n\u2022 Knowledge of vector databases (e.g., FAISS, Weaviate, Pinecone) and RAG pipelines\n\u2022 Exposure to MLOps tools or workflows (e.g., Langfuse, MLflow, Datadog, or custom monitoring tools) is a plus\n\u2022 Ability to conduct literature reviews and apply research insights to real-world systems\n\u2022 Strong communication skills and ability to work collaboratively in a fast-paced environment\n\u2022 Passion for applied AI research and building AI-native tools or systems\n\nPlease create a tailored resume that highlights relevant skills and experience for this position.", "input": "", "output": "Here is a tailored resume for the Intern AI Researcher position:\n\nSATHVIK ADDICHARLA\nDenton, TX 76207 | sathvikaddicharla@my.unt.edu | +1 (940) 597-5745 | [LinkedIn] | [GitHub]\nPROFESSIONAL SUMMARY\nAI/ML graduate student with hands-on experience in developing and fine-tuning large language models (LLMs), retrieval-augmented generation (RAG), and transformer-based applications. Skilled in integrating OpenAI and Hugging Face APIs, prompt engineering, and vector search systems (FAISS). Experienced in building AI-native tools using LangChain and LangGraph. Passionate about applied AI research with published work in IEEE and strong interest in building production-grade LLM pipelines.\nTECHNICAL SKILLS\nLLMs & Transformers: GPT, BERT, T5, LLaMA, Whisper, ELECTRA, Hugging Face Transformers\nRAG & Prompting: LangChain, LangGraph, RAG, Few-shot, Chain-of-thought, LLM-as-a-judge\nVector Search & Storage: FAISS, Pinecone (familiar), Tokenization, Embedding Optimization\nProgramming: Python, SQL, C++\nMLOps & Tooling: MLflow, Langfuse (familiar), Docker, GitHub, FastAPI, Flask\nCloud & APIs: OpenAI API, AWS S3, Hugging Face Hub, REST API\nResearch & Optimization: Literature Reviews, Quantization, Pruning, ONNX\nEDUCATION\nMaster\u2019s in Data Science\nUniversity of North Texas | 04/2024 \u2013 08/2026 | GPA: 3.66/4.0\nBachelor\u2019s in Computer Science \u2013 Data Science\nVNR Vignana Jyothi Institute of Technology, India | GPA: 8.60/10\nEXPERIENCE\nAI/ML Engineer Intern\nCantellate Software Pvt Ltd | Hyderabad, India | 03/2023 \u2013 09/2023\nDesigned and deployed real-time RAG systems using Hugging Face Transformers, LangChain, and FAISS for dynamic NLP pipelines.\nIntegrated OpenAI API into question-answering systems with prompt tuning, improving relevance and latency by 50%.\nManaged end-to-end model lifecycle: preprocessing (500GB+), model training, MLflow versioning, and AWS deployment.\nConducted benchmark evaluations of prompt engineering methods (few-shot, zero-shot, chain-of-thought) for text classification tasks.\nBuilt real-time fraud detection pipeline with sub-100ms latency using PyTorch and TorchServe; saved ~$500K monthly in risk mitigation.\nPROJECTS\nLLM Chatbot for Health Diagnostics (RAG + LangChain + LangGraph)\nBuilt a GPT-style diagnostic assistant using retrieval-augmented generation with Hugging Face and LangGraph.\nImplemented few-shot and CoT prompts with semantic vector search; monitored outputs using MLflow.\nPublished in IEEE (2024).\nAI YouTube Summarizer (Whisper + FAISS + Transformers)\nTranscribed video/audio using OpenAI\u2019s Whisper and encoded content with lightweight transformer models.\nEnabled semantic search with FAISS achieving ~92% relevance.\nReduced manual content review time by over 75%.\nSpeech Emotion Detection (LSTM + MFCC)\nBuilt deep learning model to detect speech emotions using MFCC preprocessing, LSTM architecture, and attention layers.\nFine-tuned for cross-accent robustness and interpretability.\nCERTIFICATIONS & TRAINING\nTraining LLMs with Your Own Data \u2013 DeepLearning.ai\nDeep Learning \u2013 NPTEL (IIT Ropar)\nPython for Data Science and Machine Learning \u2013 Udemy\nGoogle Data Analytics \u2013 Coursera\nPUBLICATIONS\nRAG-Based LLM Chatbot for Diagnosis Support \u2013 IEEE (09/2024)\nTransfer Learning for Autism Detection in Children \u2013 IEEE (08/2024)"}
