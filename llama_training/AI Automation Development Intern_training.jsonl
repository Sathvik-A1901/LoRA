{"instruction": "Given the following job description, create a tailored resume:\n\nJob Title: AI Automation Development Intern\n\nJob Description:\nMyDataProduct (MDP) is an independent data product studio and consultancy.\nLeveraging best-in-class talent, technologies, and techniques\u2014we design and develop data-empowered software at the intersection of:\nJobs-to-be-Done-informed product strategy\nApplied data science\nFoundational data engineering, and\nStakeholder value obsession\nEngineered with thoughtful precision, MDP data products are defined by:\nNiche verticals & use cases\nFull-job-cycle value propositions\nML-enabled capabilities\nSynergistic data integrations, and\nHyper-personalization\nWe are looking for a AI Automation Development Intern for a paid, 100% remote, 6-month internship. The expected weekly time commitment for this opportunity is 15 hours per week.\nThis opportunity may be a good fit if\nYou are comfortable working 100% remotely as part of a distributed team\nYou excel at timely execution of low-code and no-code development projects\nYour work products demonstrate a high level of attention to detail and the ability to see the big picture\nYour aptitude, mindset, and ability to learn quickly make up for shortcomings in your current skill set\nYou resonate with MDP's core values:\n....... 1. High performance, engagement, and reliability\n....... 2. Proper prioritization, timeliness, and risk mitigation\n....... 3. Outcome ownership and accountability\n....... 4. Systems thinking, modularity, and automation\n....... 5. Excellence in execution and hyper-detail orientation\n....... 6. Documentation and asynchronous work\n....... 7. Resourceful initiative\nThis role may require you to\nDesign, build, extend, and maintain workflow automations using n8n's visual and programmatic interfaces\nIntegrate various APIs and web services to create efficient and scalable automation solutions\nDevelop custom nodes and functions using JavaScript to extend n8n's capabilities\nCollaborate with cross-functional teams to understand business requirements and implement appropriate automation workflows\nDebug and optimize n8n workflows for performance and reliability\nImplement AI and machine learning integrations within n8n to enhance automation capabilities\nUse JSON to export and import n8n workflows, facilitating efficient sharing and version control of automation configurations\nDocument source code, workflow designs, research, development plans, and engineering standards to enable solution modularity, reproducibility, portability, collaboration, and knowledge sharing\nCollaborate with teammates using Slack, Figma, Google Meet, and/or other software\nAn ideal candidate has\nA publicly accessible portfolio of n8n workflow automations\nExperience with ML-related APIs and web service integrations\nExperience building full-stack web applications\nExperience using Git-based version control in a team setting\nEffective modular thinking skills evidenced by the ability to break down large, intricate undertakings into a series of smaller self-contained tasks\nExcellent written and verbal communication skills\nEmotional intelligence and coachability\nAll details of this job description are subject to change at any time, at the sole discretion of MyDataProduct.\n\nPlease create a tailored resume that highlights relevant skills and experience for this position.", "input": "", "output": "Here is a tailored resume for the AI Automation Development Intern position:\n\nSATHVIK ADDICHARLA\nDenton, TX 76207 | sathvikaddicharla@my.unt.edu | +1 (940) 597-5745 | [LinkedIn] | [GitHub]\nPROFESSIONAL SUMMARY\nAI Developer with hands-on experience designing and deploying automation pipelines and intelligent API-integrated workflows. Skilled in low-code AI integration, ML pipeline deployment, RESTful API orchestration, and modular system thinking. Proven ability to build scalable, documented solutions across cloud environments using tools like Python, JavaScript (basic), JSON, ML APIs, and no-code/low-code interfaces. Comfortable working remotely and asynchronously across distributed teams.\nTECHNICAL SKILLS\nAutomation & Integration: REST APIs, n8n (familiar), JSON, Flask, LangChain, Webhooks\nML & AI Tools: OpenAI GPT, Whisper, Hugging Face, MLflow, FAISS, Scikit-learn\nLanguages: Python, JavaScript (basic), SQL, Bash\nCloud & DevOps: AWS (S3, Lambda), Docker, GitHub, GCP (Vertex AI - exposure)\nWorkflow Management: Modular automation logic, prompt pipelines, configuration export/import, data logging\nCollaboration: Slack, Figma, Notion, Git-based versioning, Asynchronous documentation\nEDUCATION\nMaster\u2019s in Data Science\nUniversity of North Texas | 04/2024 \u2013 08/2026 | GPA: 3.66/4.0\nBachelor\u2019s in Computer Science \u2013 Data Science\nVNR Vignana Jyothi Institute of Technology, India | GPA: 8.60/10\nEXPERIENCE\nAI/ML Engineer Intern\nCantellate Software Pvt Ltd | Hyderabad, India | 03/2023 \u2013 09/2023\nDesigned modular ML workflows integrated with OpenAI APIs and REST services using Flask + Docker + MLflow.\nImplemented reusable prompt templates and API endpoints for automation of NLP tasks (retrieval, summarization).\nUsed JSON configs for pipeline tuning and performance logging; versioned artifacts via Git and API snapshots.\nCreated asynchronous response services for vector search results using FAISS and LangChain.\nCollaborated remotely with distributed engineering team using Slack, version control, and Google Meet.\nPROJECTS\nAI Chatbot + Modular API Orchestration\nDeveloped GPT-style assistant via LangChain + OpenAI + Flask REST APIs, deployable in modular microservice format.\nUsed JSON for prompt routing, semantic document retrieval, and configuration portability.\nMultimodal Content Summarization Pipeline\nWhisper + Hugging Face transformer summarization integrated with lightweight web UI; performance tracked using MLflow.\nDesigned for asynchronous workflows and integration with e-commerce or customer support tools.\nSpeech Emotion Classifier (Modular LSTM Deployment)\nDeployed audio-processing LSTM model as containerized microservice; exposed endpoints with customizable config files.\nLogged outputs and benchmarking via external JSON and documented usage for reproducibility.\nCERTIFICATIONS & TRAINING\nTraining LLMs with Your Own Data \u2013 DeepLearning.ai\nPython for Data Science \u2013 Udemy\nDeep Learning \u2013 NPTEL (IIT Ropar)\nGoogle Data Analytics \u2013 Coursera"}
